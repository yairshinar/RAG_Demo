{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo (No sentence-transformers)\n",
    "This notebook uses HuggingFace transformers only for embedding and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Colab)\n",
    "# !pip install transformers faiss-cpu torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pooling function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function\n",
    "def embed_texts(texts, tokenizer, model):\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    return mean_pooling(model_output, encoded_input['attention_mask']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 350/350 [00:00<?, ?B/s] \n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 904kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.66MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading config.json: 100%|██████████| 612/612 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.deta.configuration_deta because of the following error (look up to see its traceback):\nNo module named 'transformers.models.deta.configuration_deta'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1099\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_scipy_available\u001b[39m():\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_available\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1140\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers.models.deta.configuration_deta'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m embedding_model_name = \u001b[33m'\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m embed_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m gen_model_name = \u001b[33m'\u001b[39m\u001b[33mgoogle/flan-t5-base\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:474\u001b[39m, in \u001b[36mfrom_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m adapter_kwargs = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33madapter_kwargs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    473\u001b[39m token = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m use_auth_token = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33muse_auth_token\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    476\u001b[39m     warnings.warn(\n\u001b[32m    477\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    478\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    479\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:683\u001b[39m, in \u001b[36mkeys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    681\u001b[39m from_pretrained = replace_list_option_in_docstrings(model_mapping._model_mapping)(from_pretrained)\n\u001b[32m    682\u001b[39m \u001b[38;5;28mcls\u001b[39m.from_pretrained = \u001b[38;5;28mclassmethod\u001b[39m(from_pretrained)\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:684\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:680\u001b[39m, in \u001b[36m_load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    678\u001b[39m shortcut = checkpoint_for_example.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m    679\u001b[39m from_pretrained_docstring = from_pretrained_docstring.replace(\u001b[33m\"\u001b[39m\u001b[33mshortcut_placeholder\u001b[39m\u001b[33m\"\u001b[39m, shortcut)\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m from_pretrained.\u001b[34m__doc__\u001b[39m = from_pretrained_docstring\n\u001b[32m    681\u001b[39m from_pretrained = replace_list_option_in_docstrings(model_mapping._model_mapping)(from_pretrained)\n\u001b[32m    682\u001b[39m \u001b[38;5;28mcls\u001b[39m.from_pretrained = \u001b[38;5;28mclassmethod\u001b[39m(from_pretrained)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:625\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    620\u001b[39m use_pretrained_backbone = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33muse_pretrained_backbone\u001b[39m\u001b[33m\"\u001b[39m, config.use_pretrained_backbone)\n\u001b[32m    621\u001b[39m out_indices = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mout_indices\u001b[39m\u001b[33m\"\u001b[39m, config.out_indices)\n\u001b[32m    622\u001b[39m config = TimmBackboneConfig(\n\u001b[32m    623\u001b[39m     backbone=pretrained_model_name_or_path,\n\u001b[32m    624\u001b[39m     num_channels=num_channels,\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     features_only=features_only,\n\u001b[32m    626\u001b[39m     use_pretrained_backbone=use_pretrained_backbone,\n\u001b[32m    627\u001b[39m     out_indices=out_indices,\n\u001b[32m    628\u001b[39m )\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().from_config(config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1089\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proj\\RAG\\rag-demo-env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1101\u001b[39m, in \u001b[36m_get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.deta.configuration_deta because of the following error (look up to see its traceback):\nNo module named 'transformers.models.deta.configuration_deta'"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "# embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2' \n",
    "\n",
    "embedding_model_name = \"microsoft/mpnet-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "gen_model_name = 'google/flan-t5-base'\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_name)\n",
    "qa_pipeline = pipeline('text2text-generation', model=gen_model, tokenizer=gen_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "docs = [\n",
    "    \"ProductX is the latest widget released in 2024. It features improved battery life.\",\n",
    "    \"To reset ProductX, hold the power button for 10 seconds until the LED blinks.\",\n",
    "    \"Our support plans include Basic, Plus, and Enterprise tiers, offering 24/7 support in higher tiers.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "doc_embeddings = embed_texts(docs, tokenizer, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS index\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"How can I reset ProductX?\"\n",
    "query_vec = embed_texts([query], tokenizer, embed_model)[0]\n",
    "k = 1\n",
    "distances, indices = index.search(np.array([query_vec]), k)\n",
    "retrieved_text = docs[indices[0][0]]\n",
    "print(f'Retrieved: {retrieved_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt and generate\n",
    "prompt = f\"Context: {retrieved_text}\\nQuestion: {query}\\nAnswer:\"\n",
    "result = qa_pipeline(prompt, max_length=100)[0]['generated_text']\n",
    "print(f'Answer: {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
